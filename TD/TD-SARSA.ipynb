{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from gym_ur.game_of_ur import GoUrEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib.cm as cm\n",
    "import dill\n",
    "import os.path\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eGreedyActionSelection(env, q_curr, eps, possible_actions, movable_piece_ids):\n",
    "    '''\n",
    "    Preforms epsilon greedy action selectoin based on the Q-values.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        q_curr: A numpy array that contains the Q-values for each action for a state.\n",
    "        eps: The probability to select a random action. Float between 0 and 1.\n",
    "        \n",
    "    Returns:\n",
    "        The selected action.\n",
    "    '''\n",
    "    if np.random.rand() < (1 - eps):\n",
    "        best_action = np.argmax(q_curr)\n",
    "        if best_action in movable_piece_ids.keys():\n",
    "            return movable_piece_ids[best_action]\n",
    "        \n",
    "    return np.random.choice(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):                                                                                                                                                                                                                                           \n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):                                                                                                                                                                                                     \n",
    "        '''\n",
    "        Linear interpolation between initial_p and final_p over                                                                                                                                                                                                      \n",
    "        schedule_timesteps. After this many timesteps pass final_p is                                                                                                                                                                                                   \n",
    "        returned.                                                                                                                                                                                                                                                       \n",
    "                                                                                                                                                                                                                                                                        \n",
    "        Args:                                                                                                                                                                                                                                                    \n",
    "            - schedule_timesteps: Number of timesteps for which to linearly anneal initial_p to final_p                                                                                                                                                                                                                                                  \n",
    "            - initial_p: initial output value                                                                                                                                                                                                                                        \n",
    "            -final_p: final output value                                                                                                                                                                                                                                          \n",
    "        '''                                                                                                                                                                                                                                                       \n",
    "        self.schedule_timesteps = schedule_timesteps                                                                                                                                                                                                                    \n",
    "        self.final_p = final_p                                                                                                                                                                                                                                          \n",
    "        self.initial_p = initial_p                                                                                                                                                                                                                                      \n",
    "                                                                                                                                                                                                                                                                         \n",
    "    def value(self, t):                                                                                                                                                                                                                                                 \n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)                                                                                                                                                                                                         \n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoUrEnv(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, gamma=1.0, alpha=0.1, \n",
    "                start_eps=0.2, final_eps=0.1, annealing_steps=1000,\n",
    "                max_episode_steps=200):\n",
    "    '''\n",
    "    Sarsa algorithm.\n",
    "    \n",
    "    Args:\n",
    "        - env: The environment to train the agent on\n",
    "        - num_episodes: The number of episodes to train the agent for\n",
    "        - gamma: The discount factor\n",
    "        - alpha: The stepsize\n",
    "        - start_eps: The initial epsilon value for e-greedy action selection\n",
    "        - final_eps: The final epsilon value for the e-greedy action selection\n",
    "        - annealing_steps: The number of steps to anneal epsilon over\n",
    "        - max_episode_steps: The maximum number of steps an episode can take\n",
    "        \n",
    "    Returns: (Q_func, episode_rewards, episode_lengths)\n",
    "        - Q: Dictonary mapping state -> action values\n",
    "        - episode_rewards: Numpy array containing the reward of each episode during training\n",
    "        - episode_lengths: Numpy array containing the length of each episode during training\n",
    "    '''\n",
    "    init_q_value = 0.0\n",
    "    Q = defaultdict(lambda: np.ones(env.action_space_n) * init_q_value)\n",
    "    episode_rewards_p1 = np.zeros(num_episodes-1)\n",
    "    episode_rewards_p2 = np.zeros(num_episodes-1)\n",
    "    episode_lengths = np.zeros(num_episodes-1)\n",
    "    player1 = 0\n",
    "    player2 = 1\n",
    "    \n",
    "    exploration = LinearSchedule(annealing_steps, start_eps, final_eps)\n",
    "    \n",
    "#     time = 0\n",
    "    for i in range(num_episodes-1):\n",
    "        t = 1\n",
    "        state = env.reset()\n",
    "        reset_p1 = True\n",
    "        reset_p1 = True\n",
    "        while True:\n",
    "            # Player 1 move\n",
    "            if reset_p1:\n",
    "                reset_p1 = False\n",
    "                dice_up = env.roll()\n",
    "                possible_actions, movable_piece_ids = env.get_possible_actions(player1, dice_up)\n",
    "                if len(possible_actions) != 0:\n",
    "                    action = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                \n",
    "            new_state, reward_p1, done, _ = env.step(action)\n",
    "            \n",
    "            # Player 2 move\n",
    "            if reset_p2:\n",
    "                reset_p1 = False\n",
    "                dice_up = env.roll()\n",
    "                possible_actions, movable_piece_ids = env.get_possible_actions(player2, dice_up)\n",
    "                if len(possible_actions) != 0:\n",
    "                    action = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                \n",
    "            new_state, reward_p2, done, _ = env.step(action)\n",
    "            \n",
    "            \n",
    "            dice_up = env.roll()\n",
    "            possible_actions, movable_piece_ids = env.get_possible_actions(player1, dice_up)\n",
    "            if len(possible_actions) != 0:\n",
    "                action_dash = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                # action['piece_id'] = (player, piece_id)\n",
    "                # Therefore, use piece_id to save which piece to move in \n",
    "                # this state\n",
    "                Q[state][action['piece_id'][1]] += alpha * ((reward_p1 + (gamma * (Q[new_state][action_dash['piece_id'][1]]))) \n",
    "                                                            - Q[state][action['piece_id'][1]])\n",
    "                episode_rewards_p1[i] += reward_p1\n",
    "                state = new_state\n",
    "                action = action_dash\n",
    "                reset_p1 = False\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            # player 2 move\n",
    "            dice_up = env.roll()\n",
    "            possible_actions, movable_piece_ids = env.get_possible_actions(player2, dice_up)\n",
    "            if len(possible_actions) != 0:\n",
    "                action_dash = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                new_state, reward_p2, done, _ = env.step(action)\n",
    "                # action['piece_id'] = (player, piece_id)\n",
    "                # Therefore, use piece_id to save which piece to move in \n",
    "                # this state\n",
    "                Q[state][action['piece_id'][1]] += alpha * ((reward_p2 + (gamma * (Q[new_state][action_dash['piece_id'][1]]))) \n",
    "                                                            - Q[state][action['piece_id'][1]])\n",
    "                episode_rewards_p2[i] += reward_p2\n",
    "                state = new_state\n",
    "                action = action_dash\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            episode_lengths[i] = t\n",
    "            t += 1\n",
    "    return Q, episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  1\n",
      "1: {'piece_id': (0, 0), 'curr_pos': ('a', 5), 'next_pos': ('a', 4), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('c', 5), 'next_pos': ('c', 4), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('c', 4), 'next_pos': ('c', 3), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('c', 3), 'next_pos': ('b', 2), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('b', 2), 'next_pos': ('b', 5), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('b', 2), 'next_pos': ('b', 5), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('b', 5), 'next_pos': ('b', 7), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 0), 'curr_pos': ('b', 7), 'next_pos': ('c', 6), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 6), 'curr_pos': ('c', 5), 'next_pos': ('c', 4), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 5), 'curr_pos': ('c', 5), 'next_pos': ('c', 2), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (0, 4), 'curr_pos': ('a', 1), 'next_pos': ('b', 3), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 5), 'curr_pos': ('c', 2), 'next_pos': ('b', 3), 'double_move': False, 'replace_opp': True}\n",
      "1: {'piece_id': (1, 2), 'curr_pos': ('c', 5), 'next_pos': ('c', 3), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 2), 'curr_pos': ('c', 3), 'next_pos': ('c', 2), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 5), 'curr_pos': ('b', 3), 'next_pos': ('b', 6), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (0, 5), 'curr_pos': ('b', 2), 'next_pos': ('b', 5), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 2), 'curr_pos': ('c', 2), 'next_pos': ('c', 1), 'double_move': True, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 4), 'curr_pos': ('c', 5), 'next_pos': ('c', 2), 'double_move': False, 'replace_opp': False}\n",
      "1: {'piece_id': (1, 2), 'curr_pos': ('c', 1), 'next_pos': ('b', 1), 'double_move': False, 'replace_opp': True}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2) into shape (1,7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0399795bf3e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# train agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mq_func_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_rewards_p1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_rewards_p2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# append rewards for each run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-cb6bc4c0454d>\u001b[0m in \u001b[0;36msarsa\u001b[1;34m(env, num_episodes, gamma, alpha, start_eps, final_eps, annealing_steps, max_episode_steps)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_p1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[0mdice_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mpossible_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovable_piece_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_possible_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdice_up\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\GitHub\\game-of-ur-reinforcement-learning\\gym_ur\\game_of_ur.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[1;31m# strike of opponent's piece\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m       \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopponent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstart_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_win\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (2) into shape (1,7)"
     ]
    }
   ],
   "source": [
    "episode_rwds_p1 = []\n",
    "episode_rwds_p2 = []\n",
    "total_episodes = 100000\n",
    "dir_path = 'TD/pickels/Sarsa'\n",
    "# runs\n",
    "i = 1 \n",
    "\n",
    "while i <= 1: # 10 runs\n",
    "    \n",
    "    print(\"Run: \", i)\n",
    "    \n",
    "    # train agent\n",
    "    q_func_1, episode_rewards_p1, episode_rewards_p2, episode_lengths = sarsa(env, total_episodes)\n",
    "    \n",
    "    # append rewards for each run\n",
    "    episode_rwds_p1.append(episode_rewards_p1)\n",
    "    episode_rwds_p2.append(episode_rewards_p2)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_rwds_p1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ebb2ebc316b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp1_mean_rwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_rwds_p1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mp2_mean_rwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_rwds_p2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'episode_rwds_p1' is not defined"
     ]
    }
   ],
   "source": [
    "p1_mean_rwds = np.mean(np.array(episode_rwds_p1[-1]).reshape(1, -1), axis=0)\n",
    "p2_mean_rwds = np.mean(np.array(episode_rwds_p2[-1]).reshape(1, -1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
