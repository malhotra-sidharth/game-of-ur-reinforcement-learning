{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from gym_ur.game_of_ur import GoUrEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib.cm as cm\n",
    "import dill\n",
    "import os.path\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eGreedyActionSelection(env, q_curr, eps, possible_actions, movable_piece_ids):\n",
    "    '''\n",
    "    Preforms epsilon greedy action selectoin based on the Q-values.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment\n",
    "        q_curr: A numpy array that contains the Q-values for each action for a state.\n",
    "        eps: The probability to select a random action. Float between 0 and 1.\n",
    "        \n",
    "    Returns:\n",
    "        The selected action.\n",
    "    '''\n",
    "    if np.random.rand() < (1 - eps):\n",
    "        best_action = np.argmax(q_curr)\n",
    "        if best_action in movable_piece_ids.keys():\n",
    "            return movable_piece_ids[best_action]\n",
    "        \n",
    "    return np.random.choice(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):                                                                                                                                                                                                                                           \n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):                                                                                                                                                                                                     \n",
    "        '''\n",
    "        Linear interpolation between initial_p and final_p over                                                                                                                                                                                                      \n",
    "        schedule_timesteps. After this many timesteps pass final_p is                                                                                                                                                                                                   \n",
    "        returned.                                                                                                                                                                                                                                                       \n",
    "                                                                                                                                                                                                                                                                        \n",
    "        Args:                                                                                                                                                                                                                                                    \n",
    "            - schedule_timesteps: Number of timesteps for which to linearly anneal initial_p to final_p                                                                                                                                                                                                                                                  \n",
    "            - initial_p: initial output value                                                                                                                                                                                                                                        \n",
    "            -final_p: final output value                                                                                                                                                                                                                                          \n",
    "        '''                                                                                                                                                                                                                                                       \n",
    "        self.schedule_timesteps = schedule_timesteps                                                                                                                                                                                                                    \n",
    "        self.final_p = final_p                                                                                                                                                                                                                                          \n",
    "        self.initial_p = initial_p                                                                                                                                                                                                                                      \n",
    "                                                                                                                                                                                                                                                                         \n",
    "    def value(self, t):                                                                                                                                                                                                                                                 \n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)                                                                                                                                                                                                         \n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoUrEnv(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, gamma=1.0, alpha=0.1, \n",
    "                start_eps=0.2, final_eps=0.1, annealing_steps=1000,\n",
    "                max_episode_steps=200):\n",
    "    '''\n",
    "    Q-learning algorithm.\n",
    "    \n",
    "    Args:\n",
    "        - env: The environment to train the agent on\n",
    "        - num_episodes: The number of episodes to train the agent for\n",
    "        - gamma: The discount factor\n",
    "        - alpha: The stepsize\n",
    "        - start_eps: The initial epsilon value for e-greedy action selection\n",
    "        - final_eps: The final epsilon value for the e-greedy action selection\n",
    "        - annealing_steps: The number of steps to anneal epsilon over\n",
    "        - max_episode_steps: The maximum number of steps an episode can take\n",
    "        \n",
    "    Returns: (Q_func, episode_rewards, episode_lengths)\n",
    "        - Q: Dictonary mapping state -> action values\n",
    "        - episode_rewards: Numpy array containing the reward of each episode during training\n",
    "        - episode_lengths: Numpy array containing the length of each episode during training\n",
    "    '''\n",
    "    init_q_value = 0.0\n",
    "    Q = defaultdict(lambda: np.ones(env.action_space_n) * init_q_value)\n",
    "    episode_rewards_p1 = np.zeros(num_episodes-1)\n",
    "    episode_rewards_p2 = np.zeros(num_episodes-1)\n",
    "    episode_lengths = np.zeros(num_episodes-1)\n",
    "    player1 = 0\n",
    "    player2 = 1\n",
    "    \n",
    "    exploration = LinearSchedule(annealing_steps, start_eps, final_eps)\n",
    "    for i in range(num_episodes-1):\n",
    "        state = env.reset()\n",
    "        t = 1\n",
    "        while True:        \n",
    "            episode_lengths[i] = t\n",
    "            \n",
    "            # player 1 move\n",
    "            dice_up = env.roll()\n",
    "            possible_actions, movable_piece_ids = env.get_possible_actions(player1, dice_up)\n",
    "            if len(possible_actions) != 0:\n",
    "                action = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                new_state, reward_p1, done, _ = env.step(action)\n",
    "                episode_rewards_p1[i] += reward_p1\n",
    "                # action['piece_id'] = (player, piece_id)\n",
    "                # Therefore, use piece_id to save which piece to move in \n",
    "                # this state\n",
    "                Q[state][action['piece_id'][1]] += alpha * (reward_p1 + (gamma*max(Q[new_state])) \n",
    "                                                            - Q[state][action['piece_id'][1]])\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = new_state\n",
    "            \n",
    "            # player 2 move\n",
    "            dice_up = env.roll()\n",
    "            possible_actions, movable_piece_ids = env.get_possible_actions(player2, dice_up)\n",
    "            if len(possible_actions) != 0:\n",
    "                action = eGreedyActionSelection(env, Q[state], exploration.value(i), possible_actions, movable_piece_ids)\n",
    "                new_state, reward_p2, done, _ = env.step(action)\n",
    "                episode_rewards_p2[i] += reward_p2\n",
    "                # action['piece_id'] = (player, piece_id)\n",
    "                # Therefore, use piece_id to save which piece to move in \n",
    "                # this state\n",
    "                Q[state][action['piece_id'][1]] += alpha * (reward_p2 + (gamma*max(Q[new_state])) \n",
    "                                                            - Q[state][action['piece_id'][1]])\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = new_state\n",
    "            t += 1\n",
    "            \n",
    "                \n",
    "    return Q, episode_rewards_p1, episode_rewards_p2, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  1\n"
     ]
    }
   ],
   "source": [
    "episode_rwds_p1 = []\n",
    "episode_rwds_p2 = []\n",
    "total_episodes = 100000\n",
    "dir_path = 'TD/pickels/qLearning'\n",
    "# runs\n",
    "i = 1 \n",
    "\n",
    "while i <= 1: # 10 runs\n",
    "    \n",
    "    print(\"Run: \", i)\n",
    "    \n",
    "    # train agent\n",
    "    q_func_1, episode_rewards_p1, episode_rewards_p2, episode_lengths = q_learning(env, total_episodes)\n",
    "    \n",
    "    # append rewards for each run\n",
    "    episode_rwds_p1.append(episode_rewards_p1)\n",
    "    episode_rwds_p2.append(episode_rewards_p2)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_mean_rwds = np.mean(np.array(episode_rwds_p1[-1]).reshape(1, -1), axis=0)\n",
    "p2_mean_rwds = np.mean(np.array(episode_rwds_p2[-1]).reshape(1, -1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(p1_mean_rwds[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(p2_mean_rwds[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(p1_mean_rwds[-100:])\n",
    "plt.plot(p2_mean_rwds[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pd.Series(p1_mean_rwds).rolling(100, min_periods=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pd.Series(p2_mean_rwds).rolling(100, min_periods=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pd.Series(p1_mean_rwds[-1000:]).rolling(100, min_periods=100).mean())\n",
    "plt.plot(pd.Series(p2_mean_rwds[-1000:]).rolling(100, min_periods=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
